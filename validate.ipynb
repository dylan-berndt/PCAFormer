{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import calflops\n",
    "\n",
    "from utils import *\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location = os.path.join(\"checkpoints\", \"PCAFormer\")\n",
    "config = Config().load(os.path.join(location, \"config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KaggleHub dataset path: F:/.cache/kagglehub\\datasets\\dimensi0n\\imagenet-256\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "if \"cacheDir\" in config:\n",
    "    os.environ[\"KAGGLEHUB_CACHE\"] = config.cacheDir\n",
    "\n",
    "dataDir = download_dataset(\"dimensi0n/imagenet-256\")\n",
    "\n",
    "config.dataset.dataDir = dataDir"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def testModel(model, testSet, criterion: dict[str, nn.Module], name):\n",
    "    model.eval()\n",
    "    metrics = {}\n",
    "    # Necessary for weighted mean\n",
    "    weights = []\n",
    "    flops, macs, params = None, None, None\n",
    "    with torch.no_grad():\n",
    "        progress = 0\n",
    "        for inputs, targets in testSet:\n",
    "            if flops is None:\n",
    "                flops, macs, params = calflops.calculate_flops(model, input_shape=tuple(inputs.shape))\n",
    "\n",
    "            outputs = model(inputs.to(device))\n",
    "            for metric in criterion:\n",
    "                if metric not in metrics:\n",
    "                    metrics[metric] = []\n",
    "                metrics[metric].append(criterion[metric](outputs, targets.to(device)).item())\n",
    "            weights.append(inputs.shape[0])\n",
    "\n",
    "            progress += 1\n",
    "            print(f\"\\r{name} | {progress}/{len(testSet)} | Accuracy: {metrics['Accuracy'][-1]:.2f} | Perplexity: {metrics['Perplexity'][-1]:.2f}\", end=\"\")\n",
    "\n",
    "    weights = np.array(weights)\n",
    "    for metric in metrics:\n",
    "        metrics[metric] = np.sum(np.array(metrics[metric]) * weights) / np.sum(weights)\n",
    "\n",
    "    metrics[\"FLOPS\"] = flops\n",
    "    metrics[\"MACS\"] = macs\n",
    "    metrics[\"Params\"] = params\n",
    "\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class Accuracy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, yPred, yTrue):\n",
    "        indices = torch.argmax(yPred, dim=1)\n",
    "        return torch.mean((indices == yTrue).float())\n",
    "\n",
    "\n",
    "class Perplexity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, yPred, yTrue):\n",
    "        log = self.entropy(yPred, yTrue)\n",
    "        return torch.exp(log)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  38.92 M \n",
      "fwd MACs:                                                               677.23 GMACs\n",
      "fwd FLOPs:                                                              1.36 TFLOPS\n",
      "fwd+bwd MACs:                                                           2.03 TMACs\n",
      "fwd+bwd FLOPs:                                                          4.08 TFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "PCAFormer(\n",
      "  38.92 M = 100% Params, 677.23 GMACs = 100% MACs, 1.36 TFLOPS = 100% FLOPs\n",
      "  (patching): PatchEmbedding(\n",
      "    524.8 K = 1.35% Params, 12.88 GMACs = 1.9% MACs, 25.8 GFLOPS = 1.9% FLOPs\n",
      "    (projection): Conv2d(393.73 K = 1.01% Params, 12.88 GMACs = 1.9% MACs, 25.79 GFLOPS = 1.89% FLOPs, 3, 512, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 103.48 GMACs = 15.28% MACs, 208.08 GFLOPS = 15.29% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 168.43 MFLOPS = 0.01% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 34.49 GMACs = 5.09% MACs, 68.99 GFLOPS = 5.07% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 168.43 MFLOPS = 0.01% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 68.99 GMACs = 10.19% MACs, 138.72 GFLOPS = 10.19% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 34.49 GMACs = 5.09% MACs, 68.99 GFLOPS = 5.07% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 673.71 MFLOPS = 0.05% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 67.37 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 34.49 GMACs = 5.09% MACs, 68.99 GFLOPS = 5.07% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 69.12 GMACs = 10.21% MACs, 138.8 GFLOPS = 10.2% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 84.54 MFLOPS = 0.01% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 17.31 GMACs = 2.56% MACs, 34.63 GFLOPS = 2.54% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 84.54 MFLOPS = 0.01% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 34.63 GMACs = 5.11% MACs, 69.63 GFLOPS = 5.12% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 17.31 GMACs = 2.56% MACs, 34.63 GFLOPS = 2.54% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 338.17 MFLOPS = 0.02% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 33.82 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 17.31 GMACs = 2.56% MACs, 34.63 GFLOPS = 2.54% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 51.94 GMACs = 7.67% MACs, 104.44 GFLOPS = 7.67% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 84.54 MFLOPS = 0.01% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 17.31 GMACs = 2.56% MACs, 34.63 GFLOPS = 2.54% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 84.54 MFLOPS = 0.01% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 34.63 GMACs = 5.11% MACs, 69.63 GFLOPS = 5.12% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 17.31 GMACs = 2.56% MACs, 34.63 GFLOPS = 2.54% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 338.17 MFLOPS = 0.02% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 33.82 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 17.31 GMACs = 2.56% MACs, 34.63 GFLOPS = 2.54% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 30.47 GMACs = 4.5% MACs, 61.22 GFLOPS = 4.5% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 42.6 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 8.72 GMACs = 1.29% MACs, 17.45 GFLOPS = 1.28% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 42.6 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 17.45 GMACs = 2.58% MACs, 35.08 GFLOPS = 2.58% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 8.72 GMACs = 1.29% MACs, 17.45 GFLOPS = 1.28% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 170.39 MFLOPS = 0.01% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 17.04 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 8.72 GMACs = 1.29% MACs, 17.45 GFLOPS = 1.28% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 26.17 GMACs = 3.86% MACs, 52.63 GFLOPS = 3.87% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 42.6 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 8.72 GMACs = 1.29% MACs, 17.45 GFLOPS = 1.28% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 42.6 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 17.45 GMACs = 2.58% MACs, 35.08 GFLOPS = 2.58% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 8.72 GMACs = 1.29% MACs, 17.45 GFLOPS = 1.28% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 170.39 MFLOPS = 0.01% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 17.04 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 8.72 GMACs = 1.29% MACs, 17.45 GFLOPS = 1.28% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 14.36 GMACs = 2.12% MACs, 28.87 GFLOPS = 2.12% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 21.63 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 4.43 GMACs = 0.65% MACs, 8.86 GFLOPS = 0.65% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 21.63 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 8.86 GMACs = 1.31% MACs, 17.81 GFLOPS = 1.31% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 4.43 GMACs = 0.65% MACs, 8.86 GFLOPS = 0.65% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 86.51 MFLOPS = 0.01% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 8.65 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 4.43 GMACs = 0.65% MACs, 8.86 GFLOPS = 0.65% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 13.29 GMACs = 1.96% MACs, 26.72 GFLOPS = 1.96% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 21.63 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 4.43 GMACs = 0.65% MACs, 8.86 GFLOPS = 0.65% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 21.63 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 8.86 GMACs = 1.31% MACs, 17.81 GFLOPS = 1.31% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 4.43 GMACs = 0.65% MACs, 8.86 GFLOPS = 0.65% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 86.51 MFLOPS = 0.01% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 8.65 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 4.43 GMACs = 0.65% MACs, 8.86 GFLOPS = 0.65% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 7.11 GMACs = 1.05% MACs, 14.3 GFLOPS = 1.05% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 11.14 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 2.28 GMACs = 0.34% MACs, 4.56 GFLOPS = 0.34% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 11.14 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 4.56 GMACs = 0.67% MACs, 9.18 GFLOPS = 0.67% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 2.28 GMACs = 0.34% MACs, 4.56 GFLOPS = 0.34% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 44.56 MFLOPS = 0% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 4.46 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 2.28 GMACs = 0.34% MACs, 4.56 GFLOPS = 0.34% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 6.85 GMACs = 1.01% MACs, 13.76 GFLOPS = 1.01% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 11.14 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 2.28 GMACs = 0.34% MACs, 4.56 GFLOPS = 0.34% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 11.14 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 4.56 GMACs = 0.67% MACs, 9.18 GFLOPS = 0.67% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 2.28 GMACs = 0.34% MACs, 4.56 GFLOPS = 0.34% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 44.56 MFLOPS = 0% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 4.46 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 2.28 GMACs = 0.34% MACs, 4.56 GFLOPS = 0.34% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 3.69 GMACs = 0.55% MACs, 7.42 GFLOPS = 0.55% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 5.9 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 1.21 GMACs = 0.18% MACs, 2.42 GFLOPS = 0.18% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 5.9 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 2.42 GMACs = 0.36% MACs, 4.86 GFLOPS = 0.36% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 1.21 GMACs = 0.18% MACs, 2.42 GFLOPS = 0.18% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 23.59 MFLOPS = 0% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 2.36 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 1.21 GMACs = 0.18% MACs, 2.42 GFLOPS = 0.18% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 3.62 GMACs = 0.54% MACs, 7.29 GFLOPS = 0.54% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 5.9 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 1.21 GMACs = 0.18% MACs, 2.42 GFLOPS = 0.18% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 5.9 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 2.42 GMACs = 0.36% MACs, 4.86 GFLOPS = 0.36% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 1.21 GMACs = 0.18% MACs, 2.42 GFLOPS = 0.18% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 23.59 MFLOPS = 0% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 2.36 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 1.21 GMACs = 0.18% MACs, 2.42 GFLOPS = 0.18% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 2.03 GMACs = 0.3% MACs, 4.08 GFLOPS = 0.3% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 3.28 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 671.09 MMACs = 0.1% MACs, 1.34 GFLOPS = 0.1% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 3.28 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 1.34 GMACs = 0.2% MACs, 2.7 GFLOPS = 0.2% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 671.09 MMACs = 0.1% MACs, 1.34 GFLOPS = 0.1% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 13.11 MFLOPS = 0% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 1.31 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 671.09 MMACs = 0.1% MACs, 1.34 GFLOPS = 0.1% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer): Sequential(\n",
      "    37.88 M = 97.33% Params, 332.14 GMACs = 49.04% MACs, 667.6 GFLOPS = 49.05% FLOPs\n",
      "    (0): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 103.48 GMACs = 15.28% MACs, 208.08 GFLOPS = 15.29% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 168.43 MFLOPS = 0.01% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 34.49 GMACs = 5.09% MACs, 68.99 GFLOPS = 5.07% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 168.43 MFLOPS = 0.01% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 68.99 GMACs = 10.19% MACs, 138.72 GFLOPS = 10.19% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 34.49 GMACs = 5.09% MACs, 68.99 GFLOPS = 5.07% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 673.71 MFLOPS = 0.05% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 67.37 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 34.49 GMACs = 5.09% MACs, 68.99 GFLOPS = 5.07% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 69.12 GMACs = 10.21% MACs, 138.8 GFLOPS = 10.2% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 84.54 MFLOPS = 0.01% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 17.31 GMACs = 2.56% MACs, 34.63 GFLOPS = 2.54% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 84.54 MFLOPS = 0.01% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 34.63 GMACs = 5.11% MACs, 69.63 GFLOPS = 5.12% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 17.31 GMACs = 2.56% MACs, 34.63 GFLOPS = 2.54% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 338.17 MFLOPS = 0.02% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 33.82 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 17.31 GMACs = 2.56% MACs, 34.63 GFLOPS = 2.54% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 51.94 GMACs = 7.67% MACs, 104.44 GFLOPS = 7.67% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 84.54 MFLOPS = 0.01% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 17.31 GMACs = 2.56% MACs, 34.63 GFLOPS = 2.54% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 84.54 MFLOPS = 0.01% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 34.63 GMACs = 5.11% MACs, 69.63 GFLOPS = 5.12% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 17.31 GMACs = 2.56% MACs, 34.63 GFLOPS = 2.54% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 338.17 MFLOPS = 0.02% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 33.82 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 17.31 GMACs = 2.56% MACs, 34.63 GFLOPS = 2.54% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 30.47 GMACs = 4.5% MACs, 61.22 GFLOPS = 4.5% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 42.6 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 8.72 GMACs = 1.29% MACs, 17.45 GFLOPS = 1.28% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 42.6 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 17.45 GMACs = 2.58% MACs, 35.08 GFLOPS = 2.58% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 8.72 GMACs = 1.29% MACs, 17.45 GFLOPS = 1.28% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 170.39 MFLOPS = 0.01% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 17.04 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 8.72 GMACs = 1.29% MACs, 17.45 GFLOPS = 1.28% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 26.17 GMACs = 3.86% MACs, 52.63 GFLOPS = 3.87% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 42.6 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 8.72 GMACs = 1.29% MACs, 17.45 GFLOPS = 1.28% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 42.6 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 17.45 GMACs = 2.58% MACs, 35.08 GFLOPS = 2.58% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 8.72 GMACs = 1.29% MACs, 17.45 GFLOPS = 1.28% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 170.39 MFLOPS = 0.01% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 17.04 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 8.72 GMACs = 1.29% MACs, 17.45 GFLOPS = 1.28% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 14.36 GMACs = 2.12% MACs, 28.87 GFLOPS = 2.12% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 21.63 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 4.43 GMACs = 0.65% MACs, 8.86 GFLOPS = 0.65% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 21.63 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 8.86 GMACs = 1.31% MACs, 17.81 GFLOPS = 1.31% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 4.43 GMACs = 0.65% MACs, 8.86 GFLOPS = 0.65% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 86.51 MFLOPS = 0.01% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 8.65 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 4.43 GMACs = 0.65% MACs, 8.86 GFLOPS = 0.65% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 13.29 GMACs = 1.96% MACs, 26.72 GFLOPS = 1.96% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 21.63 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 4.43 GMACs = 0.65% MACs, 8.86 GFLOPS = 0.65% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 21.63 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 8.86 GMACs = 1.31% MACs, 17.81 GFLOPS = 1.31% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 4.43 GMACs = 0.65% MACs, 8.86 GFLOPS = 0.65% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 86.51 MFLOPS = 0.01% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 8.65 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 4.43 GMACs = 0.65% MACs, 8.86 GFLOPS = 0.65% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 7.11 GMACs = 1.05% MACs, 14.3 GFLOPS = 1.05% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 11.14 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 2.28 GMACs = 0.34% MACs, 4.56 GFLOPS = 0.34% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 11.14 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 4.56 GMACs = 0.67% MACs, 9.18 GFLOPS = 0.67% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 2.28 GMACs = 0.34% MACs, 4.56 GFLOPS = 0.34% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 44.56 MFLOPS = 0% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 4.46 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 2.28 GMACs = 0.34% MACs, 4.56 GFLOPS = 0.34% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 6.85 GMACs = 1.01% MACs, 13.76 GFLOPS = 1.01% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 11.14 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 2.28 GMACs = 0.34% MACs, 4.56 GFLOPS = 0.34% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 11.14 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 4.56 GMACs = 0.67% MACs, 9.18 GFLOPS = 0.67% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 2.28 GMACs = 0.34% MACs, 4.56 GFLOPS = 0.34% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 44.56 MFLOPS = 0% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 4.46 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 2.28 GMACs = 0.34% MACs, 4.56 GFLOPS = 0.34% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 3.69 GMACs = 0.55% MACs, 7.42 GFLOPS = 0.55% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 5.9 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 1.21 GMACs = 0.18% MACs, 2.42 GFLOPS = 0.18% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 5.9 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 2.42 GMACs = 0.36% MACs, 4.86 GFLOPS = 0.36% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 1.21 GMACs = 0.18% MACs, 2.42 GFLOPS = 0.18% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 23.59 MFLOPS = 0% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 2.36 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 1.21 GMACs = 0.18% MACs, 2.42 GFLOPS = 0.18% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 3.62 GMACs = 0.54% MACs, 7.29 GFLOPS = 0.54% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 5.9 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 1.21 GMACs = 0.18% MACs, 2.42 GFLOPS = 0.18% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 5.9 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 2.42 GMACs = 0.36% MACs, 4.86 GFLOPS = 0.36% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 1.21 GMACs = 0.18% MACs, 2.42 GFLOPS = 0.18% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 23.59 MFLOPS = 0% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 2.36 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 1.21 GMACs = 0.18% MACs, 2.42 GFLOPS = 0.18% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): PCAFormerLayer(\n",
      "      3.16 M = 8.11% Params, 2.03 GMACs = 0.3% MACs, 4.08 GFLOPS = 0.3% FLOPs\n",
      "      (ln1): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 3.28 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        1.05 M = 2.7% Params, 671.09 MMACs = 0.1% MACs, 1.34 GFLOPS = 0.1% FLOPs\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(262.66 K = 0.67% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm(1.02 K = 0% Params, 0 MACs = 0% MACs, 3.28 MFLOPS = 0% FLOPs, (512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        2.1 M = 5.41% Params, 1.34 GMACs = 0.2% MACs, 2.7 GFLOPS = 0.2% FLOPs\n",
      "        (0): Linear(1.05 M = 2.7% Params, 671.09 MMACs = 0.1% MACs, 1.34 GFLOPS = 0.1% FLOPs, in_features=512, out_features=2048, bias=True)\n",
      "        (1): LayerNorm(4.1 K = 0.01% Params, 0 MACs = 0% MACs, 13.11 MFLOPS = 0% FLOPs, (2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(0 = 0% Params, 0 MACs = 0% MACs, 1.31 MFLOPS = 0% FLOPs, approximate='none')\n",
      "        (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "        (4): Linear(1.05 M = 2.7% Params, 671.09 MMACs = 0.1% MACs, 1.34 GFLOPS = 0.1% FLOPs, in_features=2048, out_features=512, bias=True)\n",
      "        (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classification): Linear(513 K = 1.32% Params, 65.54 MMACs = 0.01% MACs, 131.07 MFLOPS = 0.01% FLOPs, in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PCAFormer | 1687/1687 | Accuracy: 0.03 | Perplexity: 711.702\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  25.56 M \n",
      "fwd MACs:                                                               523.42 GMACs\n",
      "fwd FLOPs:                                                              1.06 TFLOPS\n",
      "fwd+bwd MACs:                                                           1.57 TMACs\n",
      "fwd+bwd FLOPs:                                                          3.17 TFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNet(\n",
      "  25.56 M = 100% Params, 523.42 GMACs = 100% MACs, 1.06 TFLOPS = 100% FLOPs\n",
      "  (conv1): Conv2d(9.41 K = 0.04% Params, 15.11 GMACs = 2.89% MACs, 30.21 GFLOPS = 2.86% FLOPs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 411.04 MFLOPS = 0.04% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 205.52 MFLOPS = 0.02% FLOPs, inplace=True)\n",
      "  (maxpool): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 205.52 MFLOPS = 0.02% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    215.81 K = 0.84% Params, 85.5 GMACs = 16.33% MACs, 174.18 GFLOPS = 16.51% FLOPs\n",
      "    (0): Bottleneck(\n",
      "      75.01 K = 0.29% Params, 29.6 GMACs = 5.65% MACs, 60.53 GFLOPS = 5.74% FLOPs\n",
      "      (conv1): Conv2d(4.1 K = 0.02% Params, 1.64 GMACs = 0.31% MACs, 3.29 GFLOPS = 0.31% FLOPs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 102.76 MFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(36.86 K = 0.14% Params, 14.8 GMACs = 2.83% MACs, 29.6 GFLOPS = 2.8% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 102.76 MFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(16.38 K = 0.06% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512 = 0% Params, 0 MACs = 0% MACs, 411.04 MFLOPS = 0.04% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 308.28 MFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        16.9 K = 0.07% Params, 6.58 GMACs = 1.26% MACs, 13.56 GFLOPS = 1.29% FLOPs\n",
      "        (0): Conv2d(16.38 K = 0.06% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512 = 0% Params, 0 MACs = 0% MACs, 411.04 MFLOPS = 0.04% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      70.4 K = 0.28% Params, 27.95 GMACs = 5.34% MACs, 56.83 GFLOPS = 5.39% FLOPs\n",
      "      (conv1): Conv2d(16.38 K = 0.06% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 102.76 MFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(36.86 K = 0.14% Params, 14.8 GMACs = 2.83% MACs, 29.6 GFLOPS = 2.8% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 102.76 MFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(16.38 K = 0.06% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512 = 0% Params, 0 MACs = 0% MACs, 411.04 MFLOPS = 0.04% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 308.28 MFLOPS = 0.03% FLOPs, inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      70.4 K = 0.28% Params, 27.95 GMACs = 5.34% MACs, 56.83 GFLOPS = 5.39% FLOPs\n",
      "      (conv1): Conv2d(16.38 K = 0.06% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 102.76 MFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(36.86 K = 0.14% Params, 14.8 GMACs = 2.83% MACs, 29.6 GFLOPS = 2.8% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 102.76 MFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(16.38 K = 0.06% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512 = 0% Params, 0 MACs = 0% MACs, 411.04 MFLOPS = 0.04% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 308.28 MFLOPS = 0.03% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    1.22 M = 4.77% Params, 131.53 GMACs = 25.13% MACs, 265.35 GFLOPS = 25.15% FLOPs\n",
      "    (0): Bottleneck(\n",
      "      379.39 K = 1.48% Params, 47.68 GMACs = 9.11% MACs, 96.26 GFLOPS = 9.12% FLOPs\n",
      "      (conv1): Conv2d(32.77 K = 0.13% Params, 13.15 GMACs = 2.51% MACs, 26.31 GFLOPS = 2.49% FLOPs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256 = 0% Params, 0 MACs = 0% MACs, 205.52 MFLOPS = 0.02% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(147.46 K = 0.58% Params, 14.8 GMACs = 2.83% MACs, 29.6 GFLOPS = 2.8% FLOPs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256 = 0% Params, 0 MACs = 0% MACs, 51.38 MFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(65.54 K = 0.26% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1.02 K = 0% Params, 0 MACs = 0% MACs, 205.52 MFLOPS = 0.02% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 231.21 MFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        132.1 K = 0.52% Params, 13.15 GMACs = 2.51% MACs, 26.51 GFLOPS = 2.51% FLOPs\n",
      "        (0): Conv2d(131.07 K = 0.51% Params, 13.15 GMACs = 2.51% MACs, 26.31 GFLOPS = 2.49% FLOPs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1.02 K = 0% Params, 0 MACs = 0% MACs, 205.52 MFLOPS = 0.02% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      280.06 K = 1.1% Params, 27.95 GMACs = 5.34% MACs, 56.36 GFLOPS = 5.34% FLOPs\n",
      "      (conv1): Conv2d(65.54 K = 0.26% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256 = 0% Params, 0 MACs = 0% MACs, 51.38 MFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(147.46 K = 0.58% Params, 14.8 GMACs = 2.83% MACs, 29.6 GFLOPS = 2.8% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256 = 0% Params, 0 MACs = 0% MACs, 51.38 MFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(65.54 K = 0.26% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1.02 K = 0% Params, 0 MACs = 0% MACs, 205.52 MFLOPS = 0.02% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 154.14 MFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      280.06 K = 1.1% Params, 27.95 GMACs = 5.34% MACs, 56.36 GFLOPS = 5.34% FLOPs\n",
      "      (conv1): Conv2d(65.54 K = 0.26% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256 = 0% Params, 0 MACs = 0% MACs, 51.38 MFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(147.46 K = 0.58% Params, 14.8 GMACs = 2.83% MACs, 29.6 GFLOPS = 2.8% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256 = 0% Params, 0 MACs = 0% MACs, 51.38 MFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(65.54 K = 0.26% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1.02 K = 0% Params, 0 MACs = 0% MACs, 205.52 MFLOPS = 0.02% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 154.14 MFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      280.06 K = 1.1% Params, 27.95 GMACs = 5.34% MACs, 56.36 GFLOPS = 5.34% FLOPs\n",
      "      (conv1): Conv2d(65.54 K = 0.26% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256 = 0% Params, 0 MACs = 0% MACs, 51.38 MFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(147.46 K = 0.58% Params, 14.8 GMACs = 2.83% MACs, 29.6 GFLOPS = 2.8% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256 = 0% Params, 0 MACs = 0% MACs, 51.38 MFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(65.54 K = 0.26% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1.02 K = 0% Params, 0 MACs = 0% MACs, 205.52 MFLOPS = 0.02% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 154.14 MFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    7.1 M = 27.77% Params, 187.44 GMACs = 35.81% MACs, 376.48 GFLOPS = 35.68% FLOPs\n",
      "    (0): Bottleneck(\n",
      "      1.51 M = 5.92% Params, 47.68 GMACs = 9.11% MACs, 95.81 GFLOPS = 9.08% FLOPs\n",
      "      (conv1): Conv2d(131.07 K = 0.51% Params, 13.15 GMACs = 2.51% MACs, 26.31 GFLOPS = 2.49% FLOPs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0% Params, 0 MACs = 0% MACs, 102.76 MFLOPS = 0.01% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(589.82 K = 2.31% Params, 14.8 GMACs = 2.83% MACs, 29.6 GFLOPS = 2.8% FLOPs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0% Params, 0 MACs = 0% MACs, 25.69 MFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(262.14 K = 1.03% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2.05 K = 0.01% Params, 0 MACs = 0% MACs, 102.76 MFLOPS = 0.01% FLOPs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 115.61 MFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        526.34 K = 2.06% Params, 13.15 GMACs = 2.51% MACs, 26.41 GFLOPS = 2.5% FLOPs\n",
      "        (0): Conv2d(524.29 K = 2.05% Params, 13.15 GMACs = 2.51% MACs, 26.31 GFLOPS = 2.49% FLOPs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2.05 K = 0.01% Params, 0 MACs = 0% MACs, 102.76 MFLOPS = 0.01% FLOPs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      1.12 M = 4.37% Params, 27.95 GMACs = 5.34% MACs, 56.13 GFLOPS = 5.32% FLOPs\n",
      "      (conv1): Conv2d(262.14 K = 1.03% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0% Params, 0 MACs = 0% MACs, 25.69 MFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(589.82 K = 2.31% Params, 14.8 GMACs = 2.83% MACs, 29.6 GFLOPS = 2.8% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0% Params, 0 MACs = 0% MACs, 25.69 MFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(262.14 K = 1.03% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2.05 K = 0.01% Params, 0 MACs = 0% MACs, 102.76 MFLOPS = 0.01% FLOPs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 77.07 MFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      1.12 M = 4.37% Params, 27.95 GMACs = 5.34% MACs, 56.13 GFLOPS = 5.32% FLOPs\n",
      "      (conv1): Conv2d(262.14 K = 1.03% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0% Params, 0 MACs = 0% MACs, 25.69 MFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(589.82 K = 2.31% Params, 14.8 GMACs = 2.83% MACs, 29.6 GFLOPS = 2.8% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0% Params, 0 MACs = 0% MACs, 25.69 MFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(262.14 K = 1.03% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2.05 K = 0.01% Params, 0 MACs = 0% MACs, 102.76 MFLOPS = 0.01% FLOPs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 77.07 MFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      1.12 M = 4.37% Params, 27.95 GMACs = 5.34% MACs, 56.13 GFLOPS = 5.32% FLOPs\n",
      "      (conv1): Conv2d(262.14 K = 1.03% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0% Params, 0 MACs = 0% MACs, 25.69 MFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(589.82 K = 2.31% Params, 14.8 GMACs = 2.83% MACs, 29.6 GFLOPS = 2.8% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0% Params, 0 MACs = 0% MACs, 25.69 MFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(262.14 K = 1.03% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2.05 K = 0.01% Params, 0 MACs = 0% MACs, 102.76 MFLOPS = 0.01% FLOPs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 77.07 MFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      1.12 M = 4.37% Params, 27.95 GMACs = 5.34% MACs, 56.13 GFLOPS = 5.32% FLOPs\n",
      "      (conv1): Conv2d(262.14 K = 1.03% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0% Params, 0 MACs = 0% MACs, 25.69 MFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(589.82 K = 2.31% Params, 14.8 GMACs = 2.83% MACs, 29.6 GFLOPS = 2.8% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0% Params, 0 MACs = 0% MACs, 25.69 MFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(262.14 K = 1.03% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2.05 K = 0.01% Params, 0 MACs = 0% MACs, 102.76 MFLOPS = 0.01% FLOPs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 77.07 MFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      1.12 M = 4.37% Params, 27.95 GMACs = 5.34% MACs, 56.13 GFLOPS = 5.32% FLOPs\n",
      "      (conv1): Conv2d(262.14 K = 1.03% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0% Params, 0 MACs = 0% MACs, 25.69 MFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(589.82 K = 2.31% Params, 14.8 GMACs = 2.83% MACs, 29.6 GFLOPS = 2.8% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0% Params, 0 MACs = 0% MACs, 25.69 MFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(262.14 K = 1.03% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2.05 K = 0.01% Params, 0 MACs = 0% MACs, 102.76 MFLOPS = 0.01% FLOPs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 77.07 MFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    14.96 M = 58.55% Params, 103.58 GMACs = 19.79% MACs, 207.62 GFLOPS = 19.68% FLOPs\n",
      "    (0): Bottleneck(\n",
      "      6.04 M = 23.63% Params, 47.68 GMACs = 9.11% MACs, 95.59 GFLOPS = 9.06% FLOPs\n",
      "      (conv1): Conv2d(524.29 K = 2.05% Params, 13.15 GMACs = 2.51% MACs, 26.31 GFLOPS = 2.49% FLOPs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 K = 0% Params, 0 MACs = 0% MACs, 51.38 MFLOPS = 0% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.36 M = 9.23% Params, 14.8 GMACs = 2.83% MACs, 29.6 GFLOPS = 2.8% FLOPs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 K = 0% Params, 0 MACs = 0% MACs, 12.85 MFLOPS = 0% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1.05 M = 4.1% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(4.1 K = 0.02% Params, 0 MACs = 0% MACs, 51.38 MFLOPS = 0% FLOPs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.8 MFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        2.1 M = 8.22% Params, 13.15 GMACs = 2.51% MACs, 26.36 GFLOPS = 2.5% FLOPs\n",
      "        (0): Conv2d(2.1 M = 8.21% Params, 13.15 GMACs = 2.51% MACs, 26.31 GFLOPS = 2.49% FLOPs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(4.1 K = 0.02% Params, 0 MACs = 0% MACs, 51.38 MFLOPS = 0% FLOPs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      4.46 M = 17.46% Params, 27.95 GMACs = 5.34% MACs, 56.02 GFLOPS = 5.31% FLOPs\n",
      "      (conv1): Conv2d(1.05 M = 4.1% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 K = 0% Params, 0 MACs = 0% MACs, 12.85 MFLOPS = 0% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.36 M = 9.23% Params, 14.8 GMACs = 2.83% MACs, 29.6 GFLOPS = 2.8% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 K = 0% Params, 0 MACs = 0% MACs, 12.85 MFLOPS = 0% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1.05 M = 4.1% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(4.1 K = 0.02% Params, 0 MACs = 0% MACs, 51.38 MFLOPS = 0% FLOPs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 38.54 MFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      4.46 M = 17.46% Params, 27.95 GMACs = 5.34% MACs, 56.02 GFLOPS = 5.31% FLOPs\n",
      "      (conv1): Conv2d(1.05 M = 4.1% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 K = 0% Params, 0 MACs = 0% MACs, 12.85 MFLOPS = 0% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.36 M = 9.23% Params, 14.8 GMACs = 2.83% MACs, 29.6 GFLOPS = 2.8% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 K = 0% Params, 0 MACs = 0% MACs, 12.85 MFLOPS = 0% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1.05 M = 4.1% Params, 6.58 GMACs = 1.26% MACs, 13.15 GFLOPS = 1.25% FLOPs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(4.1 K = 0.02% Params, 0 MACs = 0% MACs, 51.38 MFLOPS = 0% FLOPs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 38.54 MFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 25.69 MFLOPS = 0% FLOPs, output_size=(1, 1))\n",
      "  (fc): Linear(2.05 M = 8.02% Params, 262.14 MMACs = 0.05% MACs, 524.29 MFLOPS = 0.05% FLOPs, in_features=2048, out_features=1000, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ResNet50 | 1687/1687 | Accuracy: 0.82 | Perplexity: 2.27\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  28.35 M \n",
      "fwd MACs:                                                               773.23 GMACs\n",
      "fwd FLOPs:                                                              1.56 TFLOPS\n",
      "fwd+bwd MACs:                                                           2.32 TMACs\n",
      "fwd+bwd FLOPs:                                                          4.67 TFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "SwinTransformer(\n",
      "  28.35 M = 100% Params, 773.23 GMACs = 100% MACs, 1.56 TFLOPS = 100% FLOPs\n",
      "  (features): Sequential(\n",
      "    27.58 M = 97.28% Params, 773.13 GMACs = 99.99% MACs, 1.56 TFLOPS = 99.98% FLOPs\n",
      "    (0): Sequential(\n",
      "      4.9 K = 0.02% Params, 2.42 GMACs = 0.31% MACs, 5.39 GFLOPS = 0.35% FLOPs\n",
      "      (0): Conv2d(4.7 K = 0.02% Params, 2.42 GMACs = 0.31% MACs, 4.88 GFLOPS = 0.31% FLOPs, 3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (1): Permute(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "      (2): LayerNorm(192 = 0% Params, 0 MACs = 0% MACs, 503.32 MFLOPS = 0.03% FLOPs, (96,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      229.83 K = 0.81% Params, 135.29 GMACs = 17.5% MACs, 274.11 GFLOPS = 17.62% FLOPs\n",
      "      (0): SwinTransformerBlockV2(\n",
      "        114.92 K = 0.41% Params, 67.65 GMACs = 8.75% MACs, 137.01 GFLOPS = 8.81% FLOPs\n",
      "        (norm1): LayerNorm(192 = 0% Params, 0 MACs = 0% MACs, 503.32 MFLOPS = 0.03% FLOPs, (96,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          40.32 K = 0.14% Params, 28.99 GMACs = 3.75% MACs, 58.39 GFLOPS = 3.75% FLOPs\n",
      "          (qkv): Linear(27.94 K = 0.1% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=96, out_features=288, bias=True)\n",
      "          (proj): Linear(9.31 K = 0.03% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=96, out_features=96, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            3.07 K = 0.01% Params, 1.15 MMACs = 0% MACs, 2.76 MFLOPS = 0% FLOPs\n",
      "            (0): Linear(1.54 K = 0.01% Params, 460.8 KMACs = 0% MACs, 921.6 KFLOPS = 0% FLOPs, in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 460.8 KFLOPS = 0% FLOPs, inplace=True)\n",
      "            (2): Linear(1.54 K = 0.01% Params, 691.2 KMACs = 0% MACs, 1.38 MFLOPS = 0% FLOPs, in_features=512, out_features=3, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm2): LayerNorm(192 = 0% Params, 0 MACs = 0% MACs, 503.32 MFLOPS = 0.03% FLOPs, (96,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          74.21 K = 0.26% Params, 38.65 GMACs = 5% MACs, 77.51 GFLOPS = 4.98% FLOPs\n",
      "          (0): Linear(37.25 K = 0.13% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=96, out_features=384, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 201.33 MFLOPS = 0.01% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(36.96 K = 0.13% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=384, out_features=96, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): SwinTransformerBlockV2(\n",
      "        114.92 K = 0.41% Params, 67.65 GMACs = 8.75% MACs, 137.11 GFLOPS = 8.81% FLOPs\n",
      "        (norm1): LayerNorm(192 = 0% Params, 0 MACs = 0% MACs, 503.32 MFLOPS = 0.03% FLOPs, (96,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          40.32 K = 0.14% Params, 28.99 GMACs = 3.75% MACs, 58.49 GFLOPS = 3.76% FLOPs\n",
      "          (qkv): Linear(27.94 K = 0.1% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=96, out_features=288, bias=True)\n",
      "          (proj): Linear(9.31 K = 0.03% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=96, out_features=96, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            3.07 K = 0.01% Params, 1.15 MMACs = 0% MACs, 2.76 MFLOPS = 0% FLOPs\n",
      "            (0): Linear(1.54 K = 0.01% Params, 460.8 KMACs = 0% MACs, 921.6 KFLOPS = 0% FLOPs, in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 460.8 KFLOPS = 0% FLOPs, inplace=True)\n",
      "            (2): Linear(1.54 K = 0.01% Params, 691.2 KMACs = 0% MACs, 1.38 MFLOPS = 0% FLOPs, in_features=512, out_features=3, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row)\n",
      "        (norm2): LayerNorm(192 = 0% Params, 0 MACs = 0% MACs, 503.32 MFLOPS = 0.03% FLOPs, (96,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          74.21 K = 0.26% Params, 38.65 GMACs = 5% MACs, 77.51 GFLOPS = 4.98% FLOPs\n",
      "          (0): Linear(37.25 K = 0.13% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=96, out_features=384, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 201.33 MFLOPS = 0.01% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(36.96 K = 0.13% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=384, out_features=96, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): PatchMergingV2(\n",
      "      74.11 K = 0.26% Params, 9.66 GMACs = 1.25% MACs, 19.58 GFLOPS = 1.26% FLOPs\n",
      "      (reduction): Linear(73.73 K = 0.26% Params, 9.66 GMACs = 1.25% MACs, 19.33 GFLOPS = 1.24% FLOPs, in_features=384, out_features=192, bias=False)\n",
      "      (norm): LayerNorm(384 = 0% Params, 0 MACs = 0% MACs, 251.66 MFLOPS = 0.02% FLOPs, (192,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      898.96 K = 3.17% Params, 125.63 GMACs = 16.25% MACs, 253.03 GFLOPS = 16.26% FLOPs\n",
      "      (0): SwinTransformerBlockV2(\n",
      "        449.48 K = 1.59% Params, 62.82 GMACs = 8.12% MACs, 126.49 GFLOPS = 8.13% FLOPs\n",
      "        (norm1): LayerNorm(384 = 0% Params, 0 MACs = 0% MACs, 251.66 MFLOPS = 0.02% FLOPs, (192,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          152.84 K = 0.54% Params, 24.16 GMACs = 3.12% MACs, 48.52 GFLOPS = 3.12% FLOPs\n",
      "          (qkv): Linear(111.17 K = 0.39% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=192, out_features=576, bias=True)\n",
      "          (proj): Linear(37.06 K = 0.13% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=192, out_features=192, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            4.61 K = 0.02% Params, 1.84 MMACs = 0% MACs, 4.15 MFLOPS = 0% FLOPs\n",
      "            (0): Linear(1.54 K = 0.01% Params, 460.8 KMACs = 0% MACs, 921.6 KFLOPS = 0% FLOPs, in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 460.8 KFLOPS = 0% FLOPs, inplace=True)\n",
      "            (2): Linear(3.07 K = 0.01% Params, 1.38 MMACs = 0% MACs, 2.76 MFLOPS = 0% FLOPs, in_features=512, out_features=6, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row)\n",
      "        (norm2): LayerNorm(384 = 0% Params, 0 MACs = 0% MACs, 251.66 MFLOPS = 0.02% FLOPs, (192,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          295.87 K = 1.04% Params, 38.65 GMACs = 5% MACs, 77.41 GFLOPS = 4.98% FLOPs\n",
      "          (0): Linear(148.22 K = 0.52% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=192, out_features=768, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 100.66 MFLOPS = 0.01% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(147.65 K = 0.52% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=768, out_features=192, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): SwinTransformerBlockV2(\n",
      "        449.48 K = 1.59% Params, 62.82 GMACs = 8.12% MACs, 126.54 GFLOPS = 8.13% FLOPs\n",
      "        (norm1): LayerNorm(384 = 0% Params, 0 MACs = 0% MACs, 251.66 MFLOPS = 0.02% FLOPs, (192,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          152.84 K = 0.54% Params, 24.16 GMACs = 3.12% MACs, 48.57 GFLOPS = 3.12% FLOPs\n",
      "          (qkv): Linear(111.17 K = 0.39% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=192, out_features=576, bias=True)\n",
      "          (proj): Linear(37.06 K = 0.13% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=192, out_features=192, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            4.61 K = 0.02% Params, 1.84 MMACs = 0% MACs, 4.15 MFLOPS = 0% FLOPs\n",
      "            (0): Linear(1.54 K = 0.01% Params, 460.8 KMACs = 0% MACs, 921.6 KFLOPS = 0% FLOPs, in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 460.8 KFLOPS = 0% FLOPs, inplace=True)\n",
      "            (2): Linear(3.07 K = 0.01% Params, 1.38 MMACs = 0% MACs, 2.76 MFLOPS = 0% FLOPs, in_features=512, out_features=6, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row)\n",
      "        (norm2): LayerNorm(384 = 0% Params, 0 MACs = 0% MACs, 251.66 MFLOPS = 0.02% FLOPs, (192,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          295.87 K = 1.04% Params, 38.65 GMACs = 5% MACs, 77.41 GFLOPS = 4.98% FLOPs\n",
      "          (0): Linear(148.22 K = 0.52% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=192, out_features=768, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 100.66 MFLOPS = 0.01% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(147.65 K = 0.52% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=768, out_features=192, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): PatchMergingV2(\n",
      "      295.68 K = 1.04% Params, 9.66 GMACs = 1.25% MACs, 19.45 GFLOPS = 1.25% FLOPs\n",
      "      (reduction): Linear(294.91 K = 1.04% Params, 9.66 GMACs = 1.25% MACs, 19.33 GFLOPS = 1.24% FLOPs, in_features=768, out_features=384, bias=False)\n",
      "      (norm): LayerNorm(768 = 0% Params, 0 MACs = 0% MACs, 125.83 MFLOPS = 0.01% FLOPs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      10.69 M = 37.72% Params, 362.41 GMACs = 46.87% MACs, 727.46 GFLOPS = 46.76% FLOPs\n",
      "      (0): SwinTransformerBlockV2(\n",
      "        1.78 M = 6.29% Params, 60.4 GMACs = 7.81% MACs, 121.23 GFLOPS = 7.79% FLOPs\n",
      "        (norm1): LayerNorm(768 = 0% Params, 0 MACs = 0% MACs, 125.83 MFLOPS = 0.01% FLOPs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          599.05 K = 2.11% Params, 21.75 GMACs = 2.81% MACs, 43.59 GFLOPS = 2.8% FLOPs\n",
      "          (qkv): Linear(443.52 K = 1.56% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(147.84 K = 0.52% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            7.68 K = 0.03% Params, 3.23 MMACs = 0% MACs, 6.91 MFLOPS = 0% FLOPs\n",
      "            (0): Linear(1.54 K = 0.01% Params, 460.8 KMACs = 0% MACs, 921.6 KFLOPS = 0% FLOPs, in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 460.8 KFLOPS = 0% FLOPs, inplace=True)\n",
      "            (2): Linear(6.14 K = 0.02% Params, 2.76 MMACs = 0% MACs, 5.53 MFLOPS = 0% FLOPs, in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row)\n",
      "        (norm2): LayerNorm(768 = 0% Params, 0 MACs = 0% MACs, 125.83 MFLOPS = 0.01% FLOPs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          1.18 M = 4.17% Params, 38.65 GMACs = 5% MACs, 77.36 GFLOPS = 4.97% FLOPs\n",
      "          (0): Linear(591.36 K = 2.09% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 50.33 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(590.21 K = 2.08% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): SwinTransformerBlockV2(\n",
      "        1.78 M = 6.29% Params, 60.4 GMACs = 7.81% MACs, 121.26 GFLOPS = 7.79% FLOPs\n",
      "        (norm1): LayerNorm(768 = 0% Params, 0 MACs = 0% MACs, 125.83 MFLOPS = 0.01% FLOPs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          599.05 K = 2.11% Params, 21.75 GMACs = 2.81% MACs, 43.62 GFLOPS = 2.8% FLOPs\n",
      "          (qkv): Linear(443.52 K = 1.56% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(147.84 K = 0.52% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            7.68 K = 0.03% Params, 3.23 MMACs = 0% MACs, 6.91 MFLOPS = 0% FLOPs\n",
      "            (0): Linear(1.54 K = 0.01% Params, 460.8 KMACs = 0% MACs, 921.6 KFLOPS = 0% FLOPs, in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 460.8 KFLOPS = 0% FLOPs, inplace=True)\n",
      "            (2): Linear(6.14 K = 0.02% Params, 2.76 MMACs = 0% MACs, 5.53 MFLOPS = 0% FLOPs, in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row)\n",
      "        (norm2): LayerNorm(768 = 0% Params, 0 MACs = 0% MACs, 125.83 MFLOPS = 0.01% FLOPs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          1.18 M = 4.17% Params, 38.65 GMACs = 5% MACs, 77.36 GFLOPS = 4.97% FLOPs\n",
      "          (0): Linear(591.36 K = 2.09% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 50.33 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(590.21 K = 2.08% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): SwinTransformerBlockV2(\n",
      "        1.78 M = 6.29% Params, 60.4 GMACs = 7.81% MACs, 121.23 GFLOPS = 7.79% FLOPs\n",
      "        (norm1): LayerNorm(768 = 0% Params, 0 MACs = 0% MACs, 125.83 MFLOPS = 0.01% FLOPs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          599.05 K = 2.11% Params, 21.75 GMACs = 2.81% MACs, 43.59 GFLOPS = 2.8% FLOPs\n",
      "          (qkv): Linear(443.52 K = 1.56% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(147.84 K = 0.52% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            7.68 K = 0.03% Params, 3.23 MMACs = 0% MACs, 6.91 MFLOPS = 0% FLOPs\n",
      "            (0): Linear(1.54 K = 0.01% Params, 460.8 KMACs = 0% MACs, 921.6 KFLOPS = 0% FLOPs, in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 460.8 KFLOPS = 0% FLOPs, inplace=True)\n",
      "            (2): Linear(6.14 K = 0.02% Params, 2.76 MMACs = 0% MACs, 5.53 MFLOPS = 0% FLOPs, in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.10909090909090911, mode=row)\n",
      "        (norm2): LayerNorm(768 = 0% Params, 0 MACs = 0% MACs, 125.83 MFLOPS = 0.01% FLOPs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          1.18 M = 4.17% Params, 38.65 GMACs = 5% MACs, 77.36 GFLOPS = 4.97% FLOPs\n",
      "          (0): Linear(591.36 K = 2.09% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 50.33 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(590.21 K = 2.08% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): SwinTransformerBlockV2(\n",
      "        1.78 M = 6.29% Params, 60.4 GMACs = 7.81% MACs, 121.26 GFLOPS = 7.79% FLOPs\n",
      "        (norm1): LayerNorm(768 = 0% Params, 0 MACs = 0% MACs, 125.83 MFLOPS = 0.01% FLOPs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          599.05 K = 2.11% Params, 21.75 GMACs = 2.81% MACs, 43.62 GFLOPS = 2.8% FLOPs\n",
      "          (qkv): Linear(443.52 K = 1.56% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(147.84 K = 0.52% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            7.68 K = 0.03% Params, 3.23 MMACs = 0% MACs, 6.91 MFLOPS = 0% FLOPs\n",
      "            (0): Linear(1.54 K = 0.01% Params, 460.8 KMACs = 0% MACs, 921.6 KFLOPS = 0% FLOPs, in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 460.8 KFLOPS = 0% FLOPs, inplace=True)\n",
      "            (2): Linear(6.14 K = 0.02% Params, 2.76 MMACs = 0% MACs, 5.53 MFLOPS = 0% FLOPs, in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1272727272727273, mode=row)\n",
      "        (norm2): LayerNorm(768 = 0% Params, 0 MACs = 0% MACs, 125.83 MFLOPS = 0.01% FLOPs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          1.18 M = 4.17% Params, 38.65 GMACs = 5% MACs, 77.36 GFLOPS = 4.97% FLOPs\n",
      "          (0): Linear(591.36 K = 2.09% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 50.33 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(590.21 K = 2.08% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): SwinTransformerBlockV2(\n",
      "        1.78 M = 6.29% Params, 60.4 GMACs = 7.81% MACs, 121.23 GFLOPS = 7.79% FLOPs\n",
      "        (norm1): LayerNorm(768 = 0% Params, 0 MACs = 0% MACs, 125.83 MFLOPS = 0.01% FLOPs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          599.05 K = 2.11% Params, 21.75 GMACs = 2.81% MACs, 43.59 GFLOPS = 2.8% FLOPs\n",
      "          (qkv): Linear(443.52 K = 1.56% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(147.84 K = 0.52% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            7.68 K = 0.03% Params, 3.23 MMACs = 0% MACs, 6.91 MFLOPS = 0% FLOPs\n",
      "            (0): Linear(1.54 K = 0.01% Params, 460.8 KMACs = 0% MACs, 921.6 KFLOPS = 0% FLOPs, in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 460.8 KFLOPS = 0% FLOPs, inplace=True)\n",
      "            (2): Linear(6.14 K = 0.02% Params, 2.76 MMACs = 0% MACs, 5.53 MFLOPS = 0% FLOPs, in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.14545454545454548, mode=row)\n",
      "        (norm2): LayerNorm(768 = 0% Params, 0 MACs = 0% MACs, 125.83 MFLOPS = 0.01% FLOPs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          1.18 M = 4.17% Params, 38.65 GMACs = 5% MACs, 77.36 GFLOPS = 4.97% FLOPs\n",
      "          (0): Linear(591.36 K = 2.09% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 50.33 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(590.21 K = 2.08% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): SwinTransformerBlockV2(\n",
      "        1.78 M = 6.29% Params, 60.4 GMACs = 7.81% MACs, 121.26 GFLOPS = 7.79% FLOPs\n",
      "        (norm1): LayerNorm(768 = 0% Params, 0 MACs = 0% MACs, 125.83 MFLOPS = 0.01% FLOPs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          599.05 K = 2.11% Params, 21.75 GMACs = 2.81% MACs, 43.62 GFLOPS = 2.8% FLOPs\n",
      "          (qkv): Linear(443.52 K = 1.56% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(147.84 K = 0.52% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            7.68 K = 0.03% Params, 3.23 MMACs = 0% MACs, 6.91 MFLOPS = 0% FLOPs\n",
      "            (0): Linear(1.54 K = 0.01% Params, 460.8 KMACs = 0% MACs, 921.6 KFLOPS = 0% FLOPs, in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 460.8 KFLOPS = 0% FLOPs, inplace=True)\n",
      "            (2): Linear(6.14 K = 0.02% Params, 2.76 MMACs = 0% MACs, 5.53 MFLOPS = 0% FLOPs, in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.16363636363636364, mode=row)\n",
      "        (norm2): LayerNorm(768 = 0% Params, 0 MACs = 0% MACs, 125.83 MFLOPS = 0.01% FLOPs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          1.18 M = 4.17% Params, 38.65 GMACs = 5% MACs, 77.36 GFLOPS = 4.97% FLOPs\n",
      "          (0): Linear(591.36 K = 2.09% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 50.33 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(590.21 K = 2.08% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): PatchMergingV2(\n",
      "      1.18 M = 4.17% Params, 9.66 GMACs = 1.25% MACs, 19.39 GFLOPS = 1.25% FLOPs\n",
      "      (reduction): Linear(1.18 M = 4.16% Params, 9.66 GMACs = 1.25% MACs, 19.33 GFLOPS = 1.24% FLOPs, in_features=1536, out_features=768, bias=False)\n",
      "      (norm): LayerNorm(1.54 K = 0.01% Params, 0 MACs = 0% MACs, 62.91 MFLOPS = 0% FLOPs, (768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      14.2 M = 50.1% Params, 118.39 GMACs = 15.31% MACs, 237.21 GFLOPS = 15.25% FLOPs\n",
      "      (0): SwinTransformerBlockV2(\n",
      "        7.1 M = 25.05% Params, 59.2 GMACs = 7.66% MACs, 118.61 GFLOPS = 7.62% FLOPs\n",
      "        (norm1): LayerNorm(1.54 K = 0.01% Params, 0 MACs = 0% MACs, 62.91 MFLOPS = 0% FLOPs, (768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          2.38 M = 8.38% Params, 20.54 GMACs = 2.66% MACs, 41.13 GFLOPS = 2.64% FLOPs\n",
      "          (qkv): Linear(1.77 M = 6.25% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=2304, bias=True)\n",
      "          (proj): Linear(590.59 K = 2.08% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            13.82 K = 0.05% Params, 5.99 MMACs = 0% MACs, 12.44 MFLOPS = 0% FLOPs\n",
      "            (0): Linear(1.54 K = 0.01% Params, 460.8 KMACs = 0% MACs, 921.6 KFLOPS = 0% FLOPs, in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 460.8 KFLOPS = 0% FLOPs, inplace=True)\n",
      "            (2): Linear(12.29 K = 0.04% Params, 5.53 MMACs = 0% MACs, 11.06 MFLOPS = 0% FLOPs, in_features=512, out_features=24, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.18181818181818182, mode=row)\n",
      "        (norm2): LayerNorm(1.54 K = 0.01% Params, 0 MACs = 0% MACs, 62.91 MFLOPS = 0% FLOPs, (768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          4.72 M = 16.66% Params, 38.65 GMACs = 5% MACs, 77.33 GFLOPS = 4.97% FLOPs\n",
      "          (0): Linear(2.36 M = 8.33% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.17 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 8.32% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): SwinTransformerBlockV2(\n",
      "        7.1 M = 25.05% Params, 59.2 GMACs = 7.66% MACs, 118.61 GFLOPS = 7.62% FLOPs\n",
      "        (norm1): LayerNorm(1.54 K = 0.01% Params, 0 MACs = 0% MACs, 62.91 MFLOPS = 0% FLOPs, (768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          2.38 M = 8.38% Params, 20.54 GMACs = 2.66% MACs, 41.13 GFLOPS = 2.64% FLOPs\n",
      "          (qkv): Linear(1.77 M = 6.25% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=2304, bias=True)\n",
      "          (proj): Linear(590.59 K = 2.08% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            13.82 K = 0.05% Params, 5.99 MMACs = 0% MACs, 12.44 MFLOPS = 0% FLOPs\n",
      "            (0): Linear(1.54 K = 0.01% Params, 460.8 KMACs = 0% MACs, 921.6 KFLOPS = 0% FLOPs, in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 460.8 KFLOPS = 0% FLOPs, inplace=True)\n",
      "            (2): Linear(12.29 K = 0.04% Params, 5.53 MMACs = 0% MACs, 11.06 MFLOPS = 0% FLOPs, in_features=512, out_features=24, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
      "        (norm2): LayerNorm(1.54 K = 0.01% Params, 0 MACs = 0% MACs, 62.91 MFLOPS = 0% FLOPs, (768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          4.72 M = 16.66% Params, 38.65 GMACs = 5% MACs, 77.33 GFLOPS = 4.97% FLOPs\n",
      "          (0): Linear(2.36 M = 8.33% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.17 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 8.32% Params, 19.33 GMACs = 2.5% MACs, 38.65 GFLOPS = 2.48% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(1.54 K = 0.01% Params, 0 MACs = 0% MACs, 62.91 MFLOPS = 0% FLOPs, (768,), eps=1e-05, elementwise_affine=True)\n",
      "  (permute): Permute(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "  (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 12.58 MFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flatten): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      "  (head): Linear(769 K = 2.71% Params, 98.3 MMACs = 0.01% MACs, 196.61 MFLOPS = 0.01% FLOPs, in_features=768, out_features=1000, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "SwinTransformer-Tiny | 1687/1687 | Accuracy: 0.85 | Perplexity: 2.01\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  86.57 M \n",
      "fwd MACs:                                                               2.16 TMACs\n",
      "fwd FLOPs:                                                              4.32 TFLOPS\n",
      "fwd+bwd MACs:                                                           6.47 TMACs\n",
      "fwd+bwd FLOPs:                                                          12.96 TFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "VisionTransformer(\n",
      "  86.57 M = 100% Params, 2.16 TMACs = 100% MACs, 4.32 TFLOPS = 100% FLOPs\n",
      "  (conv_proj): Conv2d(590.59 K = 0.68% Params, 14.8 GMACs = 0.69% MACs, 29.61 GFLOPS = 0.69% FLOPs, 3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (encoder): Encoder(\n",
      "    85.21 M = 98.43% Params, 2.14 TMACs = 99.31% MACs, 4.29 TFLOPS = 99.31% FLOPs\n",
      "    (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      85.05 M = 98.25% Params, 2.14 TMACs = 99.31% MACs, 4.29 TFLOPS = 99.3% FLOPs\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        7.09 M = 8.19% Params, 178.48 GMACs = 8.28% MACs, 357.46 GFLOPS = 8.28% FLOPs\n",
      "        (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.68% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          4.72 M = 5.46% Params, 118.98 GMACs = 5.52% MACs, 238.05 GFLOPS = 5.51% FLOPs\n",
      "          (0): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 77.46 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        7.09 M = 8.19% Params, 178.48 GMACs = 8.28% MACs, 357.46 GFLOPS = 8.28% FLOPs\n",
      "        (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.68% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          4.72 M = 5.46% Params, 118.98 GMACs = 5.52% MACs, 238.05 GFLOPS = 5.51% FLOPs\n",
      "          (0): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 77.46 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        7.09 M = 8.19% Params, 178.48 GMACs = 8.28% MACs, 357.46 GFLOPS = 8.28% FLOPs\n",
      "        (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.68% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          4.72 M = 5.46% Params, 118.98 GMACs = 5.52% MACs, 238.05 GFLOPS = 5.51% FLOPs\n",
      "          (0): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 77.46 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        7.09 M = 8.19% Params, 178.48 GMACs = 8.28% MACs, 357.46 GFLOPS = 8.28% FLOPs\n",
      "        (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.68% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          4.72 M = 5.46% Params, 118.98 GMACs = 5.52% MACs, 238.05 GFLOPS = 5.51% FLOPs\n",
      "          (0): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 77.46 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        7.09 M = 8.19% Params, 178.48 GMACs = 8.28% MACs, 357.46 GFLOPS = 8.28% FLOPs\n",
      "        (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.68% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          4.72 M = 5.46% Params, 118.98 GMACs = 5.52% MACs, 238.05 GFLOPS = 5.51% FLOPs\n",
      "          (0): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 77.46 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        7.09 M = 8.19% Params, 178.48 GMACs = 8.28% MACs, 357.46 GFLOPS = 8.28% FLOPs\n",
      "        (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.68% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          4.72 M = 5.46% Params, 118.98 GMACs = 5.52% MACs, 238.05 GFLOPS = 5.51% FLOPs\n",
      "          (0): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 77.46 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        7.09 M = 8.19% Params, 178.48 GMACs = 8.28% MACs, 357.46 GFLOPS = 8.28% FLOPs\n",
      "        (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.68% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          4.72 M = 5.46% Params, 118.98 GMACs = 5.52% MACs, 238.05 GFLOPS = 5.51% FLOPs\n",
      "          (0): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 77.46 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        7.09 M = 8.19% Params, 178.48 GMACs = 8.28% MACs, 357.46 GFLOPS = 8.28% FLOPs\n",
      "        (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.68% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          4.72 M = 5.46% Params, 118.98 GMACs = 5.52% MACs, 238.05 GFLOPS = 5.51% FLOPs\n",
      "          (0): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 77.46 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        7.09 M = 8.19% Params, 178.48 GMACs = 8.28% MACs, 357.46 GFLOPS = 8.28% FLOPs\n",
      "        (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.68% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          4.72 M = 5.46% Params, 118.98 GMACs = 5.52% MACs, 238.05 GFLOPS = 5.51% FLOPs\n",
      "          (0): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 77.46 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        7.09 M = 8.19% Params, 178.48 GMACs = 8.28% MACs, 357.46 GFLOPS = 8.28% FLOPs\n",
      "        (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.68% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          4.72 M = 5.46% Params, 118.98 GMACs = 5.52% MACs, 238.05 GFLOPS = 5.51% FLOPs\n",
      "          (0): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 77.46 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        7.09 M = 8.19% Params, 178.48 GMACs = 8.28% MACs, 357.46 GFLOPS = 8.28% FLOPs\n",
      "        (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.68% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          4.72 M = 5.46% Params, 118.98 GMACs = 5.52% MACs, 238.05 GFLOPS = 5.51% FLOPs\n",
      "          (0): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 77.46 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        7.09 M = 8.19% Params, 178.48 GMACs = 8.28% MACs, 357.46 GFLOPS = 8.28% FLOPs\n",
      "        (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.68% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          4.72 M = 5.46% Params, 118.98 GMACs = 5.52% MACs, 238.05 GFLOPS = 5.51% FLOPs\n",
      "          (0): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 77.46 MFLOPS = 0% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.73% Params, 59.49 GMACs = 2.76% MACs, 118.98 GFLOPS = 2.75% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 193.66 MFLOPS = 0% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (heads): Sequential(\n",
      "    769 K = 0.89% Params, 98.3 MMACs = 0% MACs, 196.61 MFLOPS = 0% FLOPs\n",
      "    (head): Linear(769 K = 0.89% Params, 98.3 MMACs = 0% MACs, 196.61 MFLOPS = 0% FLOPs, in_features=768, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ViT-Base-16 | 1687/1687 | Accuracy: 0.90 | Perplexity: 1.69"
     ]
    }
   ],
   "source": [
    "def loadPCA(x):\n",
    "    pcaFormer = PCAFormer(config.model)\n",
    "    pcaFormer.load_state_dict(torch.load(os.path.join(location, \"checkpoint.pt\")))\n",
    "    return pcaFormer\n",
    "\n",
    "loadResNet = lambda x: resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "loadSwin = lambda x: swin_v2_t(weights=Swin_V2_T_Weights.IMAGENET1K_V1)\n",
    "loadVIT = lambda x: vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "modelLoaders = [loadPCA, loadResNet, loadSwin, loadVIT]\n",
    "transforms = [None, ResNet50_Weights.IMAGENET1K_V1.transforms(), Swin_V2_T_Weights.IMAGENET1K_V1.transforms(),\n",
    "              ViT_B_16_Weights.IMAGENET1K_V1.transforms()]\n",
    "modelNames = [\"PCAFormer\", \"ResNet50\", \"SwinTransformer-Tiny\", \"ViT-Base-16\"]\n",
    "collectedMetrics = []\n",
    "\n",
    "criterion = {\"Accuracy\": Accuracy(), \"Perplexity\": Perplexity()}\n",
    "\n",
    "for m, modelLoader in enumerate(modelLoaders):\n",
    "    loaders = get_dataloaders(config, device, transforms[m])\n",
    "    currentModel = modelLoader(None)\n",
    "    modelMetrics = testModel(currentModel, loaders[\"val\"], criterion, modelNames[m])\n",
    "    collectedMetrics.append(modelMetrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "flops = [collectedMetric[\"FLOPS\"] for collectedMetric in collectedMetrics]\n",
    "for metric in criterion:\n",
    "    measure = [collectedMetric[metric] for collectedMetric in collectedMetrics]\n",
    "\n",
    "    data = pd.DataFrame({\"FLOPS\": flops, \"measure\": measure})\n",
    "    data = data.sort_values(\"FLOPS\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(data[\"FLOPS\"], data[\"measure\"])\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_xlabel(\"FLOPS\")\n",
    "\n",
    "    for i, name in enumerate(modelNames):\n",
    "        ax.annotate(name, (flops[i], measure[i]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(collectedMetrics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def getAttentionMatrices(m):\n",
    "    attentionMatrices = {}\n",
    "\n",
    "    def captureAttention(name):\n",
    "        def hook(module, input, output):\n",
    "            attentionWeights = output[1]\n",
    "            attentionMatrices[name] = attentionWeights\n",
    "\n",
    "        return hook\n",
    "\n",
    "    loaders = get_dataloaders(config, device, transforms[m])\n",
    "    currentModel = modelLoaders[m](None)\n",
    "    handles = []\n",
    "    if m == 3:\n",
    "        for i, block in enumerate(currentModel.encoder.layers):\n",
    "            hookHandle = block.self_attention.register_forward_hook(captureAttention(f\"layer_{i}\"))\n",
    "            handles.append(hookHandle)\n",
    "    else:\n",
    "        for i, block in enumerate(currentModel.transformer.children()):\n",
    "            hookHandle = block.attn.register_forward_hook(captureAttention(f\"layer_{i}\"))\n",
    "            handles.append(hookHandle)\n",
    "\n",
    "    inputs = next(iter(loaders[\"val\"]))[0][4, :, :, :].unsqueeze(0).to(device)\n",
    "    print(inputs.shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = currentModel(inputs)\n",
    "\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "\n",
    "    return inputs, attentionMatrices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs, pcaMatrices = getAttentionMatrices(0)\n",
    "plt.imshow(inputs.squeeze().permute(1, 2, 0).cpu())\n",
    "plt.show()\n",
    "for key, value in pcaMatrices.items():\n",
    "    print(key, value)\n",
    "    plt.imshow(value[:, 0].cpu())\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
